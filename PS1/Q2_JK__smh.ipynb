{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55301984-86bc-4613-9e60-e3eaae93d857",
   "metadata": {},
   "source": [
    "# Question 2 (BLP)\n",
    "\n",
    "We assume the demand model $$u_{ijt} = X_{jt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{jt} + \\sigma_{I}I_{i}p_{jt} + \\xi_{jt} + \\epsilon_{ijt}$$ and use BLP to estimate it. The steps are detailed below:\n",
    "\n",
    "## Step 1: Simulate draws\n",
    "\n",
    "The logit specification of our model implies that our shares can be written as an integral of fractions of utility components. Specifically, we can adapt the formulas from class to our model: $$s_{jt} = \\int \\frac{X_{jt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{jt} + \\sigma_{I}I_{it}p_{jt} + \\xi_{jt}}{1+\\sum_{m=1}^{J}X_{mt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{mt} + \\sigma_{I}I_{it}p_{mt} + \\xi_{mt}}dP_{I}(I)dP_{\\nu}(\\nu)$$ \n",
    "While this integral cannot be solved analytically, we can simulate random draws of income (uniformly over the income data we have for each product-market) and normal demand shocks to estimate shares conditional on demand parameters:\n",
    "$$\\hat{s}_{jt} = \\sum_{r}\\frac{X_{jt}\\beta + \\sigma_{B}\\nu_{r} + \\alpha p_{jt} + \\sigma_{I}I_{rt}p_{jt} + \\xi_{jt}}{1+\\sum_{m=1}^{J}X_{mt}\\beta + \\sigma_{B}\\nu_{r} + \\alpha p_{mt} + \\sigma_{I}I_{rt}p_{mt} + \\xi_{mt}}$$\n",
    "We code a function that returns this share while taking demand parameters as input below.\n",
    "\n",
    "To get closer to the notation used in the lecture notes ($\\beta$ is used differently in the problem set,) we will express the deterministic component of utility as $\\delta_{jt}$ = $X_{jt}\\beta + \\alpha p_{jt} + \\xi_{jt}$ and the variables that interact with the idiosyncratic components as $\\sigma = (\\sigma_{B},\\sigma_{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902fa90f-8c37-4b51-8c44-174554698bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>week</th>\n",
       "      <th>brand</th>\n",
       "      <th>sales_</th>\n",
       "      <th>price_</th>\n",
       "      <th>prom_</th>\n",
       "      <th>brand_2</th>\n",
       "      <th>brand_3</th>\n",
       "      <th>brand_4</th>\n",
       "      <th>brand_5</th>\n",
       "      <th>...</th>\n",
       "      <th>hhincome14</th>\n",
       "      <th>hhincome15</th>\n",
       "      <th>hhincome16</th>\n",
       "      <th>hhincome17</th>\n",
       "      <th>hhincome18</th>\n",
       "      <th>hhincome19</th>\n",
       "      <th>hhincome20</th>\n",
       "      <th>count</th>\n",
       "      <th>ms_naught</th>\n",
       "      <th>ms_by_store_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.096550</td>\n",
       "      <td>9.074136</td>\n",
       "      <td>11.56383</td>\n",
       "      <td>10.475980</td>\n",
       "      <td>11.31761</td>\n",
       "      <td>10.95628</td>\n",
       "      <td>10.73356</td>\n",
       "      <td>14181</td>\n",
       "      <td>0.994077</td>\n",
       "      <td>0.001128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.461870</td>\n",
       "      <td>10.513160</td>\n",
       "      <td>11.65730</td>\n",
       "      <td>8.476680</td>\n",
       "      <td>11.32159</td>\n",
       "      <td>10.28266</td>\n",
       "      <td>10.02471</td>\n",
       "      <td>13965</td>\n",
       "      <td>0.994271</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.676463</td>\n",
       "      <td>10.341440</td>\n",
       "      <td>11.64640</td>\n",
       "      <td>9.590904</td>\n",
       "      <td>11.96942</td>\n",
       "      <td>11.36072</td>\n",
       "      <td>11.35747</td>\n",
       "      <td>13538</td>\n",
       "      <td>0.995568</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.080500</td>\n",
       "      <td>8.894314</td>\n",
       "      <td>10.27435</td>\n",
       "      <td>11.675790</td>\n",
       "      <td>12.48191</td>\n",
       "      <td>10.79339</td>\n",
       "      <td>11.42795</td>\n",
       "      <td>13735</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>0.000874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.982330</td>\n",
       "      <td>10.211610</td>\n",
       "      <td>10.33920</td>\n",
       "      <td>11.067370</td>\n",
       "      <td>11.89204</td>\n",
       "      <td>10.28755</td>\n",
       "      <td>11.68925</td>\n",
       "      <td>13735</td>\n",
       "      <td>0.995268</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  week  brand  sales_  price_  prom_  brand_2  brand_3  brand_4  \\\n",
       "0      2     1      1      16    3.29    0.0        0        0        0   \n",
       "1      2     2      1      12    3.27    0.0        0        0        0   \n",
       "2      2     3      1       6    3.37    0.0        0        0        0   \n",
       "3      2     4      1      12    3.30    0.0        0        0        0   \n",
       "4      2     5      1      10    3.34    0.0        0        0        0   \n",
       "\n",
       "   brand_5  ...  hhincome14  hhincome15  hhincome16  hhincome17  hhincome18  \\\n",
       "0        0  ...   11.096550    9.074136    11.56383   10.475980    11.31761   \n",
       "1        0  ...   10.461870   10.513160    11.65730    8.476680    11.32159   \n",
       "2        0  ...    8.676463   10.341440    11.64640    9.590904    11.96942   \n",
       "3        0  ...   11.080500    8.894314    10.27435   11.675790    12.48191   \n",
       "4        0  ...   11.982330   10.211610    10.33920   11.067370    11.89204   \n",
       "\n",
       "   hhincome19  hhincome20  count  ms_naught  ms_by_store_week  \n",
       "0    10.95628    10.73356  14181   0.994077          0.001128  \n",
       "1    10.28266    10.02471  13965   0.994271          0.000859  \n",
       "2    11.36072    11.35747  13538   0.995568          0.000443  \n",
       "3    10.79339    11.42795  13735   0.994321          0.000874  \n",
       "4    10.28755    11.68925  13735   0.995268          0.000728  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('./cleaned_data/data.csv')\n",
    "data = data.drop(columns='Unnamed: 0')\n",
    "\n",
    "demo = pd.read_csv('./PS1_Data/OTCDemographics.csv',sep='\\t')\n",
    "data = pd.merge(data,demo,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "data = data[['store','week','brand','sales_','price_','prom_','brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11','hhincome1','hhincome2','hhincome3','hhincome4','hhincome5','hhincome6','hhincome7','hhincome8','hhincome9','hhincome10','hhincome11','hhincome12','hhincome13','hhincome14','hhincome15','hhincome16','hhincome17','hhincome18','hhincome19','hhincome20','count', 'ms_naught', 'ms_by_store_week']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "607eb459-80b8-49d3-92bc-9f5ace778f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "R = 100\n",
    "\n",
    "# Simulate draws R=100 times and average out shares\n",
    "def sim_shares(master,sigma_B,sigma_I, alpha):\n",
    "\n",
    "    # Keep covariates and intermediate variables\n",
    "    # master = master[['store','week','brand','sales_','price_','prom_','brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11','hhincome1','hhincome2','hhincome3','hhincome4','hhincome5','hhincome6','hhincome7','hhincome8','hhincome9','hhincome10','hhincome11','hhincome12','hhincome13','hhincome14','hhincome15','hhincome16','hhincome17','hhincome18','hhincome19','hhincome20','w_old','w_new','count']]\n",
    "    \n",
    "    # tot is total of all simulated shares\n",
    "    master['tot'] = 0\n",
    "\n",
    "    # Simulate draws\n",
    "    for i in range(R):\n",
    "        data_copy = master.copy()\n",
    "        \n",
    "        # Demand shock\n",
    "        nu = np.random.normal()\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "\n",
    "        data_copy['V'] = data_copy['w_old']*np.exp(sigma_B*nu + sigma_I*data_copy[f'hhincome{hh}']*data_copy['price_'])\n",
    "        # Use logit to calculate product shares in market\n",
    "        data_sum = data_copy.groupby(['store', 'week'],as_index=False)['V'].sum()\n",
    "        data_sum.rename(columns={'V':'sum'},inplace=True)\n",
    "        data_sum['sum'] = data_sum['sum'] + 1\n",
    "        data_copy = pd.merge(data_copy,data_sum,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        s = data_copy['V']/data_copy['sum']\n",
    "        master['tot'] += s\n",
    "\n",
    "    # Average share\n",
    "    master['est'] = master['tot']/R\n",
    "    return master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d9bb2-00cc-47bd-abb8-cafbfa77eca8",
   "metadata": {},
   "source": [
    "## Step 2: Contraction Mapping\n",
    "\n",
    "Given $sigma_{B}, sigma_{I}$, we can iterate the contraction mapping in the lecture notes to approximate a value of $\\delta_{jt}$ for each product-market pair that results in a share close to the actual shares in the data. The equation we iterate to approximate $\\delta_{jt}$ is:\n",
    "$$\\exp(\\delta^{i+1}_{jt}) = \\exp(\\delta^{i}_{jt})\\frac{s_{jt}^{0}}{s_{jt}(\\delta^{i}_{jt},\\beta)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b33e505-8fe5-45d9-9f48-eb4d5c9d7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta: deterministic component of jt-level utility\n",
    "def calc_delta(orig,sigma_B,sigma_I,alpha):\n",
    "    # Initialize search values and threshold\n",
    "    epsilon = 0.00001\n",
    "    orig['w_old'] = np.exp(orig['ms_by_store_week'])\n",
    "    orig['w_new'] = 0\n",
    "    count = 0\n",
    "\n",
    "    # Iterate contraction mapping until threshold is found\n",
    "    while True:\n",
    "        if count > 100:\n",
    "            print(np.average((orig['ms_by_store_week'] - orig['est']).abs()))\n",
    "            print(orig['est'])\n",
    "            break\n",
    "        if any(orig['w_old'].isnull()):\n",
    "            raise Exception(\"NaNs\")\n",
    "        # orig['delta'] = np.log(orig['w_old'])\n",
    "        orig = sim_shares(orig,sigma_B,sigma_I, alpha)\n",
    "        orig['w_new'] = orig['w_old']*orig['ms_by_store_week']/orig['est']\n",
    "        \n",
    "        if np.average((orig['ms_by_store_week'] - orig['est']).abs()) < epsilon:\n",
    "            break\n",
    "        orig['w_old'] = orig['w_new']\n",
    "        count += 1\n",
    "\n",
    "    return np.log(orig['w_new']).to_numpy()\n",
    "\n",
    "# Calculate xi: our jt-level residual\n",
    "# Two parts: iterate contraction mapping, then subtract out linear terms given beta\n",
    "def calc_xi(orig,sigma_B,sigma_I,beta, alpha):\n",
    "\n",
    "    # Calculate delta\n",
    "    orig['delta'] = calc_delta(orig,sigma_B,sigma_I, alpha)    \n",
    "\n",
    "    # Calculate xi: take linear component out of delta\n",
    "    orig['xi'] = orig['delta'] - orig[['price_','brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11']].dot(beta)\n",
    "    return orig['xi'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c7330-4fd0-488d-8921-9ff8bf654ac0",
   "metadata": {},
   "source": [
    "Below I try to run the above code step by step. Keeping the mess so that you can see the (lack of) convergence: the outputted numbers are the difference between successive values of $\\delta_{jt}$, averaged over all jt-pairs. It looks like there's initial convergence, but eventually it starts to explode..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b7129ec-8d12-4ca8-80b2-6fd8cc41bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_is = [i*0.01 for i in range(3)]\n",
    "sigma_bs = [-i*0.01 for i in range(3)]\n",
    "alphas = [-i*0.5 for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90bcac83-943d-4e8a-b7c6-11479516f702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "master = data.copy()\n",
    "\n",
    "xis = [[[None for _ in sigma_is] for _ in sigma_bs] for _ in alphas]\n",
    "min_xi_val = 10000000\n",
    "min_xi_loc = (10000,10000,10000)\n",
    "for i, ii in enumerate(sigma_is):\n",
    "    for b, bb in enumerate(sigma_bs):\n",
    "        for a, aa in enumerate(alphas):\n",
    "            try:\n",
    "                xi = calc_xi(master,bb , ii, np.ones(11), alpha=aa) \n",
    "                xis[i][b][a] = xi\n",
    "                if sum(xi_i**2 for xi_i in xi) < min_xi_val:\n",
    "                    min_xi_val = sum(xi_i**2 for xi_i in xi)\n",
    "                    min_xi_loc = tuple([i, b, a])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n",
    "min_xi_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c860fab-b643-41c1-bc1f-6c62fa595491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6967617.636429625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0.001128\n",
       "1        0.000861\n",
       "2        0.000447\n",
       "3        0.000882\n",
       "4        0.000736\n",
       "           ...   \n",
       "38539    0.000732\n",
       "38540    0.000277\n",
       "38541    0.000314\n",
       "38542    0.000506\n",
       "38543    0.000361\n",
       "Name: est, Length: 38544, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(min_xi_val)\n",
    "master['est']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00642da7-1fa8-43b9-84bf-d41d9f7e530f",
   "metadata": {},
   "source": [
    "## Step 3: Define GMM objective function\n",
    "\n",
    "We now use our instruments to define an objective function which is to be minimized to find our optimal paramters $\\beta$, $\\sigma_{B}$, and $\\sigma_{I}$. Using the formula found in Nevo's RA guide, we can express $\\beta$ as a function of $(\\sigma_{B},\\sigma_{I})$: \n",
    "$$\\beta = (X^{T}ZWZ^{T}X)^{-1}X^{T}ZWZ^{T}\\delta(\\sigma_{B},\\sigma_{I})$$  \n",
    "With $\\beta$ in hand, we can now calculate $\\xi(\\sigma_{B},\\sigma_{I},\\beta)$ and thus our entire objective function:\n",
    "$$\\xi^{T}ZWZ^{T}\\xi$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85ea135d-4ca7-42d9-b493-614237ee3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = pd.read_csv('./PS1_Data/OTCDataInstruments.csv',sep='\\t')\n",
    "instr = instr.drop(columns=['store','week','brand','avoutprice'])\n",
    "Z = instr.to_numpy()\n",
    "W = np.linalg.inv(np.matmul(np.transpose(Z),Z))\n",
    "X = orig[['price_','brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11']].to_numpy()\n",
    "\n",
    "def gmm_obj(sigma):\n",
    "    sigma_B = sigma[0]\n",
    "    sigma_I = sigma[1]\n",
    "    proj = np.linalg.inv(np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),X)))))\n",
    "    vect = np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),calc_delta(data,sigma_B,sigma_I)))))\n",
    "    beta = np.matmul(proj,vect)\n",
    "    xi = calc_xi(data,sigma_B,sigma_I,beta)\n",
    "    ans = np.matmul(np.transpose(xi),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),xi))))\n",
    "    first_comp = np.array([1,0])\n",
    "    second_comp = np.array([0,1])\n",
    "    return [np.matmul(first_comp,ans),np.matmul(second_comp,ans)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9278fee-ccf7-4b69-a168-5ba6dd071f6c",
   "metadata": {},
   "source": [
    "## Step 4: Nonlinear search over parameters\n",
    "\n",
    "Now that we've defined a loss function to minimize, we look for parameters $\\sigma_{B}, \\sigma_{I}$ that minimize it. We use scipy's fsolve, which relies on MINPACK's hybrid algorithm, for nonlinear optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f01aeb-2e1a-40f8-94be-71204e437dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "(sigma_B,sigma_I) = fsolve(gmm_obj,[1,1])\n",
    "proj = np.linalg.inv(np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),X)))))\n",
    "vect = np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),calc_delta(data,sigma_B,sigma_I)))))\n",
    "beta = np.matmul(proj,vect)\n",
    "alpha = beta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b15ef-0570-4bba-bd20-42c371ab137d",
   "metadata": {},
   "source": [
    "## Elasticity calculation\n",
    "\n",
    "Unlike the logit specification, elasticities under BLP need to be simulated. We will simulate: \n",
    "$$ e_{jjt} = -\\frac{p_{jt}}{s_{jt}}\\int (\\alpha + \\sigma_{I}I_{i})Pr_{ijt}(1-Pr_{ijt})dP_{D}(D)dP_{\\nu}(\\nu) $$ $$ e_{jkt} = \\frac{p_{kt}}{s_{jt}} \\int (\\alpha + \\sigma_{I}I_{i})Pr_{ijt}Pr_{ikt}dP_{D}(D)dP_{\\nu}(\\nu)$$\n",
    "where $Pr_{ijt}$ is the probability of $i$ choosing $j$, simulated using the procedure in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14efd9-e514-4883-a0c0-aede5976b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta for each jt-pair\n",
    "data['delta'] = calc_delta(data,sigma_B,sigma_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559faaa-43ee-4b0d-b89c-405a4f3421c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_own_e(orig,alpha,sigma_B,sigma_I):\n",
    "\n",
    "    # e_own is total of all simulated share-price derivatives\n",
    "    orig['e_own'] = 0\n",
    "\n",
    "    # Simulate draws\n",
    "    for i in range(R):\n",
    "        data_copy = master.copy()\n",
    "        \n",
    "        # Demand shock\n",
    "        nu = np.random.normal()\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "        \n",
    "        data_copy['V'] = data_copy.apply(calc_V,axis=1,args=(sigma_B,sigma_I,nu,hh))\n",
    "\n",
    "        # Use logit to calculate product shares in market\n",
    "        data_sum = data_copy.groupby(['store', 'week'],as_index=False)['V'].sum()\n",
    "        data_sum.rename(columns={'V':'sum'},inplace=True)\n",
    "        data_sum['sum'] = data_sum['sum'] + 1\n",
    "        data_copy = pd.merge(data_copy,data_sum,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        data_copy['s'] = data_copy['V']/data_copy['sum']\n",
    "        data_copy['dsdp'] = data_copy['s']*(1-data_copy['s'])*(alpha + sigma_I*data_copy['hhincome'+str(hh)])\n",
    "        data_copy = data_copy[['store','week','brand','dsdp']]\n",
    "\n",
    "        # Add this iteration to our total\n",
    "        orig = pd.merge(orig,data_copy,how='left',left_on=['store','week','brand'],right_on=['store','week','brand'],validate='1:1')\n",
    "        orig['e_own'] = orig['e_own'] + orig['dsdp']\n",
    "        orig = orig.drop(columns=['dsdp'])\n",
    "\n",
    "    # Calculate elasticity\n",
    "    orig['e_own'] = -1*orig['price_']*orig['e_own']/R\n",
    "    orig['e_own'] = orig['e_own']*orig['sales_']/orig['count']\n",
    "    return orig\n",
    "\n",
    "\n",
    "def calc_cross_e(orig,alpha,sigma_I,k):\n",
    "\n",
    "    orig['e_'+str(k)] = 0\n",
    "    \n",
    "    for i in range(R):\n",
    "        data_copy = orig.copy()\n",
    "        \n",
    "        # Demand shock\n",
    "        nu = np.random.normal()\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "\n",
    "        # Calculate utility\n",
    "        data_copy['V'] = data_copy.apply(calc_V,axis=1,args=(sigma_B,sigma_I,nu,hh))\n",
    "\n",
    "        # Use logit to calculate product shares in market\n",
    "        data_sum = data_copy.groupby(['store', 'week'],as_index=False)['V'].sum()\n",
    "        data_sum.rename(columns={'V':'sum'},inplace=True)\n",
    "        data_sum['sum'] = data_sum['sum'] + 1\n",
    "        data_copy = pd.merge(data_copy,data_sum,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        data_copy['s'] = data_copy['V']/data_copy['sum']\n",
    "\n",
    "        data_k = orig[orig['brand']==k]\n",
    "        data_k = data_k.rename(columns={'s':'s_k'})\n",
    "        data_k = data_k[['store','week','brand','s_k']]\n",
    "        data_copy = pd.merge(data_copy,data_k,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        \n",
    "        data_copy['dsdp'] = data_copy['s']*(data_copy['s_k'])*(alpha + sigma_I*data_copy['hhincome'+str(hh)])\n",
    "        data_copy = data_copy[['store','week','brand','dsdp']]\n",
    "\n",
    "        # Add this iteration to our total\n",
    "        orig = pd.merge(orig,data_copy,how='left',left_on=['store','week','brand'],right_on=['store','week','brand'],validate='1:1')\n",
    "        orig['e_'+str(k)] = orig['e_'+str(k)] + orig['dsdp']\n",
    "        orig = orig.drop(columns=['dsdp'])\n",
    "\n",
    "    data_k = orig[orig['brand']==k]\n",
    "    data_k = data_k.rename(columns={'price_':'price_k'})\n",
    "    data_k = data_k[['store','week','price_k']]\n",
    "    orig = pd.merge(orig,data_k,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "    orig['e_'+str(k)] = orig['e_'+str(k)]*orig['price_k']*orig['sales_']/(orig['count']*R)\n",
    "    \n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6791fe-77f5-4b50-ac85-23b3a2df1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b: Elasticities for store 9, week 10\n",
    "data = calc_own_e(data,alpha,sigma_B,sigma_I)\n",
    "for i in range(1,12):\n",
    "    data = calc_cross_e(data,alpha,sigma_I,i)\n",
    "\n",
    "data_ans = data[((data['week'] == 10) & (data['store'] == 9))]\n",
    "data_ans = data_ans[['brand','e_own','e_1','e_2','e_3','e_4','e_5','e_6','e_7','e_8','e_9','e_10','e_11']]\n",
    "for i in range(1,12):\n",
    "    data[data['brand']==i]['e_'+str(i)] = data['e_own']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65698bce-48c1-49b1-8a61-aae0758ba9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
