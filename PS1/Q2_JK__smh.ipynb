{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55301984-86bc-4613-9e60-e3eaae93d857",
   "metadata": {},
   "source": [
    "# Question 2 (BLP)\n",
    "\n",
    "We assume the demand model $$u_{ijt} = X_{jt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{jt} + \\sigma_{I}I_{i}p_{jt} + \\xi_{jt} + \\epsilon_{ijt}$$ and use BLP to estimate it. The steps are detailed below:\n",
    "\n",
    "## Step 1: Simulate draws\n",
    "\n",
    "The logit specification of our model implies that our shares can be written as an integral of fractions of utility components. Specifically, we can adapt the formulas from class to our model: $$s_{jt} = \\int \\frac{X_{jt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{jt} + \\sigma_{I}I_{it}p_{jt} + \\xi_{jt}}{1+\\sum_{m=1}^{J}X_{mt}\\beta + \\sigma_{B}\\nu_{i} + \\alpha p_{mt} + \\sigma_{I}I_{it}p_{mt} + \\xi_{mt}}dP_{I}(I)dP_{\\nu}(\\nu)$$ \n",
    "While this integral cannot be solved analytically, we can simulate random draws of income (uniformly over the income data we have for each product-market) and normal demand shocks to estimate shares conditional on demand parameters:\n",
    "$$\\hat{s}_{jt} = \\sum_{r}\\frac{X_{jt}\\beta + \\sigma_{B}\\nu_{r} + \\alpha p_{jt} + \\sigma_{I}I_{rt}p_{jt} + \\xi_{jt}}{1+\\sum_{m=1}^{J}X_{mt}\\beta + \\sigma_{B}\\nu_{r} + \\alpha p_{mt} + \\sigma_{I}I_{rt}p_{mt} + \\xi_{mt}}$$\n",
    "We code a function that returns this share while taking demand parameters as input below.\n",
    "\n",
    "To get closer to the notation used in the lecture notes ($\\beta$ is used differently in the problem set,) we will express the deterministic component of utility as $\\delta_{jt}$ = $X_{jt}\\beta + \\alpha p_{jt} + \\xi_{jt}$ and the variables that interact with the idiosyncratic components as $\\sigma = (\\sigma_{B},\\sigma_{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fa90f-8c37-4b51-8c44-174554698bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "data = pd.read_csv('./cleaned_data/data.csv')\n",
    "data = data.drop(columns='Unnamed: 0')\n",
    "\n",
    "demo = pd.read_csv('./PS1_Data/OTCDemographics.csv',sep='\\t')\n",
    "data = pd.merge(data,demo,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "data = data[['store','week','brand','sales_','price_','prom_', 'brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11','hhincome1','hhincome2','hhincome3','hhincome4','hhincome5','hhincome6','hhincome7','hhincome8','hhincome9','hhincome10','hhincome11','hhincome12','hhincome13','hhincome14','hhincome15','hhincome16','hhincome17','hhincome18','hhincome19','hhincome20','count', 'ms_naught', 'ms_by_store_week']]\n",
    "\n",
    "# data['branded'] = 1 - any(data[[f'brand_{i}' for i in range(10,12)]])\n",
    "data.head()\n",
    "\n",
    "instr = pd.read_csv('./PS1_Data/OTCDataInstruments.csv',sep='\\t')\n",
    "instr = instr.drop(columns=['store','week','brand','avoutprice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607eb459-80b8-49d3-92bc-9f5ace778f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "R = 100\n",
    "\n",
    "# Simulate draws R=100 times and average out shares\n",
    "def sim_shares(master,sigma_B,sigma_I):\n",
    "\n",
    "    # Keep covariates and intermediate variables\n",
    "    master = master[['store','week','brand','ms_by_store_week','sales_','price_','prom_','brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11','hhincome1','hhincome2','hhincome3','hhincome4','hhincome5','hhincome6','hhincome7','hhincome8','hhincome9','hhincome10','hhincome11','hhincome12','hhincome13','hhincome14','hhincome15','hhincome16','hhincome17','hhincome18','hhincome19','hhincome20','w_old','w_new','count']]\n",
    "    \n",
    "    # tot is total of all simulated shares\n",
    "    # master['tot'] = 0\n",
    "    total = pd.Series([0 for _ in range(len(master))])\n",
    "    \n",
    "    nus = np.random.standard_normal(R)\n",
    "    # Simulate draws\n",
    "    for i in range(R):\n",
    "        data_copy = master.copy()\n",
    "        # Demand shock\n",
    "        nu = nus[i]\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "\n",
    "        data_copy['V'] = data_copy['w_old']*np.exp(sigma_B*nu + sigma_I*data_copy[f'hhincome{hh}']*data_copy['price_'])\n",
    "        # Use logit to calculate product shares in market\n",
    "        data_sum = data_copy.groupby(['store', 'week'],as_index=False)['V'].sum()\n",
    "        data_sum.rename(columns={'V':'sum'},inplace=True)\n",
    "        data_sum['sum'] = data_sum['sum'] + 1\n",
    "        data_copy = pd.merge(data_copy,data_sum,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        s = data_copy['V']/data_copy['sum']\n",
    "        total += s\n",
    "\n",
    "    # Average share\n",
    "    master['est'] = total/R\n",
    "    return master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d9bb2-00cc-47bd-abb8-cafbfa77eca8",
   "metadata": {},
   "source": [
    "## Step 2: Contraction Mapping\n",
    "\n",
    "Given $sigma_{B}, sigma_{I}$, we can iterate the contraction mapping in the lecture notes to approximate a value of $\\delta_{jt}$ for each product-market pair that results in a share close to the actual shares in the data. The equation we iterate to approximate $\\delta_{jt}$ is:\n",
    "$$\\exp(\\delta^{i+1}_{jt}) = \\exp(\\delta^{i}_{jt})\\frac{s_{jt}^{0}}{s_{jt}(\\delta^{i}_{jt},\\beta)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33e505-8fe5-45d9-9f48-eb4d5c9d7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta: deterministic component of jt-level utility\n",
    "def calc_delta(orig,sigma_B,sigma_I,delta=None):\n",
    "    # Initialize search values and threshold\n",
    "    epsilon = 0.01\n",
    "    orig['w_old'] = np.exp(delta) if delta is not None else np.exp(orig['ms_by_store_week'])\n",
    "    orig['w_new'] = 0\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate contraction mapping until threshold is found\n",
    "    while True:\n",
    "        if any(orig['w_old'].isnull()):\n",
    "            print(count)\n",
    "            raise Exception(\"NaNs\")\n",
    "\n",
    "        orig = sim_shares(orig,sigma_B,sigma_I)\n",
    "        orig['w_new'] = orig['w_old']*orig['ms_by_store_week']/orig['est']\n",
    "        \n",
    "        if np.average(np.log(orig['ms_by_store_week']/orig['est']).abs()) < epsilon:\n",
    "            break\n",
    "        if count > 100:\n",
    "            print(\"over_count\")\n",
    "            print(np.average(np.log(orig['ms_by_store_week']/orig['est'])))\n",
    "            break\n",
    "        orig['w_old'] = orig['w_new']\n",
    "        count += 1\n",
    "\n",
    "    return np.log(orig['w_new']).to_numpy()\n",
    "\n",
    "# Calculate xi: our jt-level residual\n",
    "# Two parts: iterate contraction mapping, then subtract out linear terms given beta\n",
    "def calc_xi(data,beta): \n",
    "\n",
    "    # Calculate xi: take linear component out of delta\n",
    "    data['xi'] = data['delta'] - data[['price_', 'brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11']].dot(beta)\n",
    "    return data['xi'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c7330-4fd0-488d-8921-9ff8bf654ac0",
   "metadata": {},
   "source": [
    "Below I try to run the above code step by step. Keeping the mess so that you can see the (lack of) convergence: the outputted numbers are the difference between successive values of $\\delta_{jt}$, averaged over all jt-pairs. It looks like there's initial convergence, but eventually it starts to explode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00642da7-1fa8-43b9-84bf-d41d9f7e530f",
   "metadata": {},
   "source": [
    "## Step 3: Define GMM objective function\n",
    "\n",
    "We now use our instruments to define an objective function which is to be minimized to find our optimal paramters $\\beta$, $\\sigma_{B}$, and $\\sigma_{I}$. Using the formula found in Nevo's RA guide, we can express $\\beta$ as a function of $(\\sigma_{B},\\sigma_{I})$: \n",
    "$$\\beta = (X^{T}ZWZ^{T}X)^{-1}X^{T}ZWZ^{T}\\delta(\\sigma_{B},\\sigma_{I})$$  \n",
    "With $\\beta$ in hand, we can now calculate $\\xi(\\sigma_{B},\\sigma_{I},\\beta)$ and thus our entire objective function:\n",
    "$$\\xi^{T}ZWZ^{T}\\xi$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea135d-4ca7-42d9-b493-614237ee3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = data.copy()\n",
    "\n",
    "#kxn\n",
    "Z = instr.to_numpy()\n",
    "\n",
    "# nxn\n",
    "W = np.linalg.inv(np.matmul(np.transpose(Z),Z))\n",
    "\n",
    "# 12xn\n",
    "X = master[['price_', 'brand_2','brand_3','brand_4','brand_5','brand_6','brand_7','brand_8','brand_9','brand_10','brand_11']].to_numpy()\n",
    "\n",
    "#12x12\n",
    "proj = np.linalg.inv(np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),X)))))\n",
    "\n",
    "def gmm_obj(sigma, data):\n",
    "    sigma_B = sigma[0]\n",
    "    sigma_I = sigma[1]\n",
    "\n",
    "    if 'delta' in data.columns:\n",
    "        delta=data['delta']\n",
    "    else:\n",
    "        delta = None\n",
    "    data['delta'] = calc_delta(data,sigma_B,sigma_I,delta)\n",
    "    #1x1\n",
    "    vect = np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),data['delta']))))\n",
    "    beta = np.matmul(proj,vect)\n",
    "    # 1Xn\n",
    "    xi = calc_xi(data,beta)\n",
    "\n",
    "    # 1x1 \n",
    "    ans = np.matmul(np.transpose(xi),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),xi))))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9278fee-ccf7-4b69-a168-5ba6dd071f6c",
   "metadata": {},
   "source": [
    "~## Step 4: Nonlinear search over parameters~\n",
    "\n",
    "~Now that we've defined a loss function to minimize, we look for parameters $\\sigma_{B}, \\sigma_{I}$ that minimize it. We use scipy's fsolve, which relies on MINPACK's hybrid algorithm, for nonlinear optimization.~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd523b-0e17-4a91-872f-cc0aa596baf0",
   "metadata": {},
   "source": [
    "## Step 4: Linear Search over parameters\n",
    "\n",
    "Scipy's non-linear minimize model does not funtion well when there is systematic variation. Switch, for the sake of finding $\\sigma_{B}, \\sigma_{I}$ to a linear search model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f69ac2-0425-4c91-8a3e-255d5a0787ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "def search_grid(data, range_b, range_i, scale_b, scale_i):\n",
    "    mses = {}\n",
    "    best = 100000\n",
    "    sigma_is = np.arange(range_i[0], range_i[1], scale_i)\n",
    "    sigma_bs = np.arange(range_b[0], range_b[1], scale_b)\n",
    "    for sigma_i in sigma_is:\n",
    "        for sigma_b in sigma_bs:\n",
    "            try:\n",
    "                mse = gmm_obj([sigma_b, sigma_i], data)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception {e}\")\n",
    "                continue\n",
    "            if best > mse:\n",
    "                best = mse\n",
    "                print(best)\n",
    "                best_coef = (sigma_b, sigma_i)\n",
    "            mses[(sigma_b, sigma_i)] = mse\n",
    "    return (best, best_coef, mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ba4d8-8ac6-4e46-bcc6-9da73713ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best, best_coef, mses) = search_grid(data, (0, 2), (0,2), 0.5,0.5)\n",
    "(best, best_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308220cd-6ab8-4784-b9c4-6d035bff21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sigma_B, sigma_I] = best_coef\n",
    "range_B = (sigma_B -0.5, sigma_B + 0.5)\n",
    "range_I = (sigma_I -0.5, sigma_I + 0.5)\n",
    "(best, best_coef, mses) = search_grid(data, range_B,range_I, 0.1,  0.1)\n",
    "(best, best_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09a471-a571-4c0f-8792-137a987a254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sigma_B, sigma_I] = best_coef\n",
    "range_B = (sigma_B -0.1, sigma_B + 0.1)\n",
    "range_I = (sigma_I -0.1, sigma_I + 0.1)\n",
    "(best, best_coef, mses) = search_grid(data, range_B,range_I, 0.01,  0.01)\n",
    "(best, best_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867a701-9053-459e-9420-c762b678f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sigma_B, sigma_I] = best_coef\n",
    "range_B = (sigma_B -0.01, sigma_B + 0.01)\n",
    "range_I = (sigma_I -0.01, sigma_I + 0.01)\n",
    "(best, best_coef, mses) = search_grid(data, range_B,range_I, 0.001,  0.001)\n",
    "(best, best_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c9522-4f1c-4f32-b0b6-b41684d2e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sigma_B, sigma_I] = best_coef\n",
    "\n",
    "proj = np.linalg.inv(np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),X)))))\n",
    "\n",
    "vect = np.matmul(np.transpose(X),np.matmul(Z,np.matmul(W,np.matmul(np.transpose(Z),calc_delta(data,sigma_I,sigma_B, data['delta'])))))\n",
    "\n",
    "\n",
    "beta = np.matmul(proj,vect)\n",
    "alpha = beta[0]\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b15ef-0570-4bba-bd20-42c371ab137d",
   "metadata": {},
   "source": [
    "## Elasticity calculation\n",
    "\n",
    "Unlike the logit specification, elasticities under BLP need to be simulated. We will simulate: \n",
    "$$ e_{jjt} = -\\frac{p_{jt}}{s_{jt}}\\int (\\alpha + \\sigma_{I}I_{i})Pr_{ijt}(1-Pr_{ijt})dP_{D}(D)dP_{\\nu}(\\nu) $$ $$ e_{jkt} = \\frac{p_{kt}}{s_{jt}} \\int (\\alpha + \\sigma_{I}I_{i})Pr_{ijt}Pr_{ikt}dP_{D}(D)dP_{\\nu}(\\nu)$$\n",
    "where $Pr_{ijt}$ is the probability of $i$ choosing $j$, simulated using the procedure in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14efd9-e514-4883-a0c0-aede5976b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,12):\n",
    "    data.loc[data['brand'] == i, f'price_{i}'] = data.loc[data['brand'] == i, 'price_']\n",
    "    data.loc[data['brand'] != i, f'price_{i}'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559faaa-43ee-4b0d-b89c-405a4f3421c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "def calc_own_e(orig,alpha,sigma_B,sigma_I):\n",
    "\n",
    "    # e_own is total of all simulated share-price derivatives\n",
    "    orig['running_e_own'] = 0\n",
    "\n",
    "    # Simulate draws\n",
    "    for i in range(R):\n",
    "        data_copy = orig.copy()\n",
    "        \n",
    "        # Demand shock\n",
    "        nu = np.random.normal()\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "    \n",
    "        data_copy['dsdp'] = (alpha + sigma_I*data_copy['hhincome'+str(hh)])*data_copy['ms_by_store_week']*(1-data_copy['ms_by_store_week'])\n",
    "        data_copy = data_copy[['store','week','brand','dsdp']]\n",
    "\n",
    "        # # Add this iteration to our total\n",
    "        orig = pd.merge(orig,data_copy,how='left',left_on=['store','week','brand'],right_on=['store','week','brand'],validate='1:1')\n",
    "        orig['running_e_own'] = orig['running_e_own'] + orig['dsdp']\n",
    "        orig = orig.drop(columns=['dsdp'])\n",
    "\n",
    "    # Calculate elasticity\n",
    "    orig['e_own'] = orig['price_']*orig['running_e_own']/(orig['ms_by_store_week']*R)\n",
    "    return orig\n",
    "\n",
    "\n",
    "def calc_cross_e(orig,alpha,sigma_I,k):\n",
    "    orig[f'e_{k}'] = 0\n",
    "    \n",
    "    for i in range(R):\n",
    "        data_copy = orig.copy()\n",
    "        \n",
    "        # Demand shock\n",
    "        nu = np.random.normal()\n",
    "        # Choose income randomly\n",
    "        hh = random.randint(1,20)\n",
    "\n",
    "        # Calculate utility\n",
    "        data_copy['V'] = data_copy['w_old']*np.exp(sigma_B*nu + sigma_I*data_copy[f'hhincome{hh}']*data_copy['price_'])\n",
    "\n",
    "        # Use logit to calculate product shares in market\n",
    "        data_sum = data_copy.groupby(['store', 'week'],as_index=False)['V'].sum()\n",
    "        data_sum.rename(columns={'V':'sum'},inplace=True)\n",
    "        data_sum['sum'] = data_sum['sum'] + 1\n",
    "        data_copy = pd.merge(data_copy,data_sum,how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        data_copy['s'] = data_copy['V']/data_copy['sum']\n",
    "\n",
    "        data_k = orig[orig['brand']==k]\n",
    "        data_k = data_k.rename(columns={'ms_by_store_week':'s_k'})\n",
    "        data_k = data_k[['store','week','brand','s_k']]\n",
    "        data_copy = pd.merge(data_copy,data_k.drop(columns=['brand']),how='left',left_on=['store','week'],right_on=['store','week'],validate='m:1')\n",
    "        data_copy['dsdp'] = data_copy['s']*(data_copy['s_k'])*(alpha + sigma_I*data_copy['hhincome'+str(hh)])\n",
    "        data_copy = data_copy[['store','week','brand','dsdp']]\n",
    "\n",
    "        # Add this iteration to our total\n",
    "        orig = pd.merge(orig,data_copy,how='left',left_on=['store','week','brand'],right_on=['store','week','brand'],validate='1:1')\n",
    "        orig[f'e_{k}'] = orig[f'e_{k}'] + orig['dsdp']\n",
    "        print(orig[f'e_{k}'])\n",
    "        orig = orig.drop(columns=['dsdp'])\n",
    "    \n",
    "    orig[f'e_{k}'] = orig[f'e_{k}']*orig[f'price_{k}']/(orig['ms_by_store_week']/R)\n",
    "    \n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6791fe-77f5-4b50-ac85-23b3a2df1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b: Elasticities for store 9, week 10\n",
    "alpha = beta[0]\n",
    "\n",
    "data = calc_own_e(data,alpha,sigma_B,sigma_I)\n",
    "for i in range(1,12):\n",
    "    data = calc_cross_e(data,alpha,sigma_I,i)\n",
    "\n",
    "data_ans = data_ans[['brand','e_own','e_1','e_2','e_3','e_4','e_5','e_6','e_7','e_8','e_9','e_10','e_11']]\n",
    "for i in range(1,12):\n",
    "    data.loc[data['brand']==i, f'e_{i}'] = data.loc[data['brand']==i, 'e_own']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345d9a2-e507-43dd-be32-fb0d95325edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_elasticities = data.groupby('brand')[[f'e_{i}' for i in range(1,12)]].aggregate('mean')\n",
    "price_elasticities\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
